# Finetuning Language Models
Playing around with fine-tuning LLMs with techniques like LoRA. 

## Relevant Resources
[Alpaca-LoRA Repo](https://github.com/tloen/alpaca-lora/)
[Decapoda Weights](https://huggingface.co/decapoda-research/llama-7b-hf)
[GPT in 60 lines of numpy](https://jaykmody.com/blog/gpt-from-scratch/)
